{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip -q install aimodelshare"
      ],
      "metadata": {
        "id": "84MVywpRvJlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from aimodelshare import download_data\n",
        "import onnxruntime as ort\n",
        "import nltk\n",
        "from scipy.sparse import csr_matrix\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import joblib\n",
        "import onnxruntime\n",
        "import onnx\n",
        "import onnx\n",
        "from data_preprocessing import to_series\n",
        "from data_preprocessing import Preprocessor\n",
        "from data_preprocessing import LemmatizedTokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OjlqpbUbksP",
        "outputId": "6bf83ba6-6b5b-4655-a05d-f1b307223831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime\n",
        "import pandas as pd\n",
        "\n",
        "# Load the ONNX model\n",
        "model = onnxruntime.InferenceSession('/content/gridmodel.onnx')\n",
        "\n",
        "# Assuming `tokenizer` and `to_series` are defined functions\n",
        "# Load the test data\n",
        "X_test = pd.read_csv('test_sample.csv')\n",
        "X_train=to_series(X_train)\n",
        "X_test=to_series(X_test)\n",
        "y_train_labels=to_series(y_train_labels)\n",
        "\n",
        "# Preprocess the test data\n",
        "X_test_series = to_series(X_test.transpose())\n",
        "\n",
        "# Tokenize the test data\n",
        "X_test_tfidf = tokenizer.transform(X_test_series)\n",
        "\n",
        "# Assuming input name is 'float_input'\n",
        "input_name = model.get_inputs()[0].name\n",
        "\n",
        "# Run inference\n",
        "output = model.run(None, {input_name: X_test_tfidf_dense.astype('float32')})\n",
        "\n",
        "# Reshape the output array to 2D\n",
        "output_2d = np.reshape(output[0], (len(output[0]), 1))\n",
        "\n",
        "# Get the index of the maximum value along axis 1\n",
        "predictions = np.argmax(output_2d, axis=1)\n",
        "\n",
        "# Convert predictions to labels\n",
        "labels = ['negative', 'positive']\n",
        "predicted_labels = [labels[i] for i in predictions]\n",
        "\n",
        "# Print the predictions\n",
        "print(predicted_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "IXWWtk2rqtj5",
        "outputId": "5170536f-f9b0-4ffe-e400-77858e9a3cb4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'LemmatizedTokenizer' object has no attribute 'vectorizer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-0edf0a201f0a>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Tokenize the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mX_test_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_series\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Assuming input name is 'float_input'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/data_preprocessing.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'LemmatizedTokenizer' object has no attribute 'vectorizer'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)"
      ],
      "metadata": {
        "id": "1kJfs2LnuQZX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}